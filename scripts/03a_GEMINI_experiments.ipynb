{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBiLa: Perturbation Experiment with Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I set up the perturbation experiments for Gemini. First, the general infrastructure for interacting with Gemini is set up. Second, the prompt is designed as well as the iterative API requests to Google AI studios to generate the answers to the according questions and answer options. In a third and last step, the answers of the model are stored in a dataframe for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "import dataclasses\n",
    "import typing_extensions as typing\n",
    "\n",
    "genai.configure(api_key= os.environ[\"API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# get different gemini models\n",
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Gemini model to use\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\",\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this section I import the preprocessed WVS dataset which functions as the relevant input for the model because it contains the relevant questions and (perturbated) answer options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Design and Pipeline\n",
    "In this section I define the prompt with which the model is fed. This prompt takes selected questions from the WVS as well as their answer options and their different perturbations. From this input the model is instructed to created json-formatted output for easier further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Answer the following question: {question}\n",
    "This is a list of possible answer options: {perturbed_options}\n",
    "\n",
    "Provide the answer in this JSON format:\n",
    "{{\n",
    "  \"question_ID\": \"{question_ID}\",\n",
    "  \"answer\": \"{{answer}}\"\n",
    "}}\n",
    "\"\"\"\n",
    "# Return: list[Recipe]\"\"\"\n",
    "raw_response = model.generate_content(prompt)\n",
    "response = json.loads(raw_response.text)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hubila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
